---
layout: default
title: The Supervised Machine Learning book
tagline: Andreas Lindholm, Niklas Wahlström, Fredrik Lindsten and Thomas B. Schön
description: An upcoming textbook
---

When we developed the course [Statistical Machine Learning](http://www.it.uu.se/edu/course/homepage/sml/) for engineering students at Uppsala University, we found no appropriate textbook, so we ended up writing our own. It will be published by [Cambridge University Press](https://www.cambridge.org/) in 2021.

[Andreas Lindholm](http://www.it.uu.se/katalog/andsv164/),
[Niklas Wahlström](https://www.it.uu.se/katalog/nikwa778/),
[Fredrik Lindsten](https://liu.se/medarbetare/freli29), and
[Thomas B. Schön](http://user.it.uu.se/~thosc112/)

A draft of the book is available on this page. **We will keep a PDF of the book freely available also after its publication.**


[**Latest draft of the book**](sml-book-latest-draft.pdf)

## Table of Contents

1. **Introduction** (not in draft yet)
2. **Supervised machine learning: a first approach**
   - Supervised learning
   - A distance-based method: k-NN
   - A rule-based method: Decision trees
3. **Basic parametric models for regression and classification**
   - Linear regression
   - Logistic regression
   - Nonlinear input transformations and regularization (not in draft yet)
   - Nonlinear parametric models (not in draft yet)
4. **Understanding, evaluating and improving the performance**
   - Expected new data error: performance in production
   - Estimating the expected new data error
   - The training error–generalization gap decomposition
   - The bias-variance decomposition
   - Evaluation for imbalanced and asymmetric classification problems
5. **Learning parametric models**
   - Regularization
   - Loss functions
   - Parameter optimization
   - Optimization with large datasets
6. **Neural networks and deep learning**
   - Neural networks
   - Convolutional neural networks
   - Training a neural network
7. **Ensemble methods: Bagging and boosting**
   - Bagging
   - Random forests
   - Boosting and AdaBoost
   - Gradient boosting
8. **Nonlinear input transformations and kernels**
   - Creating features by nonlinear input transformations (not in draft yet)
   - The kernel trick in linear regression and support vector regression (not in draft yet)
   - Kernel k-NN and kernel theory (not in draft yet)
   - Support vector classification (not in draft yet)
9. **The Bayesian approach and Gaussian processes** 
   - Bayesian linear regression (not in draft yet)
   - Gaussian processes (not in draft yet)
10. **User aspects of machine learning**
    - Defining the machine learning problem
    - Improving a machine learning model
    - What if you cannot collect more data?
    - Practical data issues
    - Can I trust my machine learning model? (not in draft yet)
    - Ethics in machine learning (not in draft yet)
11. **Generative models and learning from unlabeled data**
    - Generative models: LDA, QDA and more
    - Semi-supervised and self-supervised learning (not in draft yet)
    - Unsupervised learning (not in draft yet)


## Exercise material

Will eventually be added to this page. Meanwhile you may have a look at the material for our [course at Uppsala University](http://www.it.uu.se/edu/course/homepage/sml/) (exercise material will be available at the course beginning in January).

## [Report mistakes and give feedback](https://github.com/uu-sml/sml-book-page/issues)
(A free GitHub account is required)
